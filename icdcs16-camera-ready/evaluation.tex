
\section{Evaluation}
\label{sec:evaluation}
In this section, we present an experimental evaluation of \projecttitle based on the implementation described in  $\S$\ref{sec:implementation}. 
Our evaluation answers the following questions.

\input{plotsTex/overheads}

\begin{itemize}
\item What performance overheads does \projecttitle impose for recording the provenance graph? ($\S$\ref{subsec:overheads})
\item What are the sources for these overheads? ($\S$\ref{subsec:performance-overheads-breakdown})
\item How do these overheads scale with increase in the size of the input data? ($\S$\ref{subsec:data-sizes-overheads})
\item What are the space overheads for the CPG? ($\S$\ref{subsec:overheads-breakdown})
\end{itemize}




\myparagraph{Experimental platform} We used an Intel Xeon processor based on Broadwell micro-architecture
as our host machine. The
host system consists of $8$ cores ($16$ hyper-threads) of Intel(R) Xeon(R) CPU Processor D-$1540$
($12$M Cache, $2.00$ GHz) and $32$ GB of DRAM main memory. The host
machine is running Linux with kernel $4.3.0$ in $64$-bit mode. 


\myparagraph{Applications and dataset}  We evaluated \projecttitle using applications from two multithreaded benchmark suites: Phoenix 2.0 \cite{phoenix} and PARSEC 3.0 \cite{parsec}. Table~\ref{tab:apps} lists the applications used for the evaluation along with the input data and benchmark parameters.


\myparagraph{Performance metrics: Time and Work}  For each run, we consider two types of measures: \emph{time} and
 {\em work}.  Time refers to the amount of (end-to-end)
run-time to complete the parallel computation.  Work refers to the total amount of
computation performed by all threads and is measured as the overall CPUs utilization for all threads.  


\myparagraph{Measurements} All applications were compiled using
GCC $5.2.1$ compiler with {\tt -o3} optimization flag. For all
measurements, we report the average over $6$ runs with minimum and maximum values
discarded (truncated mean).

 We measured work and time numbers for both \pthreads and \projecttitle executions with the same number of threads. For time measurements, we report the run-time comparison between the native {\tt pthreads} execution, and \projecttitle execution.   To measure work, we used the CPU accounting controller in {\tt cgroups} to account the CPU usage of all threads. 

Finally, the log produced by
{\tt perf} was written to {\tt /tmp} on {\tt tmpfs} to allow high throughput.

\input{plotsTex/overheads-breakdown}

\myparagraph{Additional results}  Due to the space limitation, the work measurements are covered in a technical report~\cite{inspector-techreport} and also available here: \href{https://mic92.github.io/inspector/index.html}{web-link}.





\subsection{Performance Overheads for Data Provenance}
\label{subsec:overheads}

First, we explain the provenance overheads imposed by \projecttitle w.r.t. the native \pthreads execution. Figure~\ref{fig:overheads} shows the provenance overheads of \projecttitle w.r.t. the native \pthreads execution with varying number of
threads (from $2$ to $16$ threads). As expected, the provenance overheads increases with the increase in the number of threads. This is because the shared memory commit ($\S$\ref{sec:impl-lib}) takes longer time with a higher number of threads, as each thread spends less time computing on the input data.  



The experiment shows that the provenance overheads using \projecttitle vary across applications. 
We observe that a majority of applications ($9$/$12$) have a reasonable overhead between $1\times$ up to $2.5\times$ w.r.t. the native execution. However, three applications have exceptionally high overheads:  {\em canneal}, {\em reverse\_index}, and {\em kmeans}. The high overheads is explained as follows: {\em canneal} modifies a lot of memory pages that leads to a high number of page faults for deriving read and write sets (see Table~\ref{tab:apps}). Whereas, {\em reverse\_index} does a lot of small memory allocations across threads leading to a large number of segmentation faults 
(details omitted --- see  \href{https://mic92.github.io/inspector/index.html\#measurement_table}{web-link}).  Finally, {\em kmeans} creates more than $400$ threads until the clusters co-efficient converges, when we specify $500$ as the parameter for the iterative convergence algorithm (see Table~\ref{tab:apps}). Since, creating a process takes more time than creating a thread, we see a slowdown in {\em kmeans}.


On the other hand, {\em linear\_regression} performs better than \pthreads, which is explained by the fact that our implementation of threads as processes ($\S$\ref{sec:implementation})  avoids false sharing, as previously noted by Sheriff~\cite{false-sharing-sheriff}, which leads to improved performance. 



Lastly, in the case of {\em streamcluster}, we were limited by our physical memory to store the log in {\tt tmpfs} for $16$ threads (see $\S$\ref{subsec:overheads-breakdown}). Therefore, we also show the overheads with $14$ and $15$ threads,  where the provenance log can fit into the main memory.  To better understand the breakdown of provenance, we chose $15$ threads for {\em streamcluster} in $\S$\ref{subsec:performance-overheads-breakdown}.




\subsection{Performance Overheads Breakdown} 
\label{subsec:performance-overheads-breakdown}

Next, we investigated the breakdown of the provenance overheads. Recall that our system implementation has two major components: (1) the threading library ($\S$\ref{sec:impl-lib}), and (2) the OS support for \intelpt ($\S$\ref{sec:impl-OS}). 
Figure~\ref{fig:overheads-breakdown} shows the breakdown of overheads with $16$ threads normalized to the native \pthreads execution. We quantify the breakdown as the time taken by the threading library  and the OS support for \intelpt  . The result shows an interesting pattern: the applications with unreasonably high overheads ({\em canneal}, {\em reverse\_index}, and {\em kmeans}) spend a majority of time in the threading library for the above mentioned reasons. Whereas, the overheads for tracing the control flow due to \intelpt  is a dominant factor for the other applications. These results highlight that for a majority of applications ($9/12$) the underlying hardware is still a bottleneck to achieve low provenance overheads.  


\subsection{Scalability with the Input Data}
\label{subsec:data-sizes-overheads}

\input{tables/benches_mix}
 In addition to scalability w.r.t.  threads, we also measured the performance overheads with increase in the size of the input data.  For that, we
report the performance overheads for four applications that are available with
three input sizes: small ($S$), medium ($M$), and large ($L$). These four applications are: {\em histogram}, {\em linear\_regression}, {\em string\_match}, and {\em word\_count}.

In this experiment, we kept the number of threads to a constant  ($16$ threads), and we varied the input sizes for these applications.  Figure~\ref{fig:data-size-overheads} shows the results for our experiment. The bar plot shows the performance overheads w.r.t. to the native \pthreads execution on the $Y1$-axis for three input sizes ($S$, $M$, $L$). For the reference, the input sizes are also shown by a line plot in the same figure on the $Y2$-axis. 

The result shows that the gap between \pthreads and \projecttitle narrows with bigger input sizes. This is due to the fact that most applications use a data-parallel programming design pattern for parallelization, where the main threads divides the input data evenly between the worker threads. As the input size increases, each thread needs to perform more work (or compute on a larger input size) than the time spent for synchronization. As a result, each thread spends relatively more time outside the shared-memory commit to compute on the data, and thus, it results in improved performance.
 



\subsection{Space Overheads for the Provenance Graph}
\label{subsec:overheads-breakdown}

Finally, we present the space overhead for storing the provenance graph. A major limitation of using \intelpt is that it produces a large amounts of trace data. Furthermore, the threading library also produces trace data to record the data and schedule dependencies.  Table~\ref{tab:space-overheads} shows the space overhead for all applications with~$16$ threads. Note that we report the combined space overheads for \projecttitle, the individual breakdown between the threading library and the OS support for \intelpt is available online:  \href{https://mic92.github.io/inspector/index.html\#measurement_table}{web-link}. 

The space overheads vary across applications: it can be as low as $183$MB for {\em linear\_regression} and as high as $29.3$GB for {\em streamcluster}. The result shows a strong correlation between the log bandwidth and branch instructions with a correlation coefficient of 0.89, which was expected, because the
log consists of taken branches.

Fortunately,  the provenance log written by {\tt perf} turns out to be highly compressible. We
were able to achieve a compression ratio of between $6\times$ and $37\times$ times using the $lz4$ compression algorithm. 
Furthermore, the snapshot facility (described in $\S$\ref{sec:snapshot}) restricts the active area of space usage, and the user can reuse the space in the ring buffer after analyzing (or collecting) the provenance graph.


\input{plotsTex/data-size-overheads}




