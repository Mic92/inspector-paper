\section{Evaluation}
\label{sec:evaluation}

In this section, we present an experimental evaluation of \projecttitle based on the implementation described in  \secref{implementation}. Our evaluation answers the following questions.

\begin{itemize}
\item What overheads does \projecttitle impose for recording data provenance? ($\S$~\ref{subsec:overheads})
\item How do these overheads scale with increases in the size of input data? ($\S$~\ref{subsec:data-sizes-overheads})
\item What are the sources for the provenance overheads? ($\S$~\ref{subsec:overheads-breakdown})
\end{itemize}

Before answering these questions, we first describe the experimental setup used for the evaluation.

\subsection{Experimental Setup}


\myparagraph{Experimental platform} We used an Intel Xeon processor based
multicore architecture as our host machine for our evaluation. The
host system consists of 8 cores (16 hyperthreads) of Intel(R) Xeon(R) CPU Processor D-1540
(12M Cache, 2.00 GHz) and 32 GB of DRAM main memory. The host
machine is running Linux with kernel 4.3.0 in 64-bit mode. The log produced by
{\tt perf} was written to {\tt /tmp} on {\tt tmpfs} to allow high throughput.
%When varying the number of threads, we also regulate the numbers of active cpu cores.
%No additional core was provided for Perf.


\myparagraph{Applications and dataset}  We evaluated \projecttitle with applications from two multithreaded benchmark suites: Phoenix 2.0 \cite{phoenix} and PARSEC 3.0 \cite{parsec}. Table~\ref{tab:apps} lists the applications used for the evaluation.% along with the input data and benchmark parameters.


\myparagraph{Measurements}  For all experiments,  we report {\tt time}
measurements, i.e., run-time comparison, between the native {\tt pthreads}
execution, and \projecttitle execution.  All applications were compiled using
GCC 5.2.1 compiler with -$o3$ optimization flag. For all performance
measurements, we report the average over 6 runs with minimum and maximum values
discarded (truncated mean).

\myparagraph{Additional results} Due to the space limitation, we only present  time measurements for all experiments. We also measured  {\em work} numbers, i.e., the overall CPUs utilization for all threads for both \pthreads and \projecttitle executions. (To measure work, we used the CPU accounting controller in {\tt cgroups} to account the CPU usage of all threads.) The work measurements are available online on the following anonymized link:  \href{https://goo.gl/UgPNdS}{goo.gl/UgPNdS}


%\myparagraph{Metrics: Time and Work}  We consider two types of measures to report the performance metrics: {\em time} and {\em work}. In a nutshell, time measurements reflect the end user perceived latency, whereas work measurements assess the overall resource (CPU) utilization.  More specifically,  time refers to the end-to-end computation time for the multithreaded applications. Work refers to the total computation performed by all threads, and it is measured as the cumulative CPU time for all threads. To measure work, we used the CPU accounting controller in {\tt cgroups} to account the CPU usage of all threads.

%\myparagraph{Measurements} All applications were compiled using GCC 5.2.1 compiler with -$o3$ optimization flag. For all performance measurements, we report the average over 10 runs with minimum and maximum values discarded.

%\myparagraph{Performance metrics: Work and Time}  For each run, we consider two types of measures: \emph{work} and
% {\em time}. Work refers to the total amount of
%computation performed by all threads and is measured as the total
%run-time of all threads. Time refers to the amount of (end-to-end)
%run-time to complete the parallel computation. Both metrics are important
%and complementary: time measurements reflect the end user perceived latency,
%whereas work measurements assess the overall resource (CPU) utilization.

\subsection{Provenance Overheads}
\label{subsec:overheads}

Figure~\ref{fig:overheads} shows the provenance overheads of \projecttitle with respect to the native \pthreads execution with varying number of
threads (from $12$ to $64$ threads). We observe, that most benchmarks have an
overhead between 1x and 2.5x, when recording data provenance.

The overhead of collecting processor trace while using native
\pthreads was 2 slower on average and there for causes an
significant part of the of the slowdown.

In {\tt Canneal} and {\tt Reverse Index} a high number of pages were modified.
Reverse index does a lot of small memory allocations across the threads. This
leads to more segmentation faults, because these allocations are located on
different pages. In Kmeans during execution over 1000 threads were created. As
processes take more time to create instead of threads, we see a slowdown here as
well.

For linear regression a speedup was measured. Because in this execution model,
every “thread” has its own virtual memory: No cohearence protocol has to be run
between commits. This prevents false sharing which results in a speedup even
when recording the execution trace.

The provenance overheads increases with the increase in the number of threads.
This was expected, because the serial phase between memory commits takes longer,
with a higher number of threads. In Streamcluster, we were hitting the memory
limit to store the log in {\tt tmpfs} for 16 threads. To better understand the
breakdown we chose 15 threads in Figure~\ref{fig:overheads}, where the log can
fit into the main memory. \input{plotsTex/overheads}


\subsection{Scalability w.r.t. Input Data Sizes}
\label{subsec:data-sizes-overheads}

To assess the performance of \projecttitle under different input sizes, we
report the performance overheads for the benchmarks, which are available with
three input sizes: small ($S$), medium ($M$), and large ($L$).
Figure~\ref{fig:data-size-overheads} shows the performance overheads with

These result shows the gap between \pthreads and \projecttitle narrows with
bigger input sizes. The algorithms used here divide input evenly between the
threads. As the input size increase therefor the parallel phase increases and
the overhead of the memory commit gets smaller in relatively.


\input{plotsTex/data-size-overheads}


\subsection{Overheads Breakdown}
\label{subsec:overheads-breakdown}

Figure~\ref{fig:overheads-breakdown} shows the overheads breakdown with $16$ threads....


\input{plotsTex/overheads-breakdown}


We also measured the overheads breakdown by
quantifying the , and the space for he execution trace. Table~\ref{tab:apps} shows the performance breakdown for all applications  with~$16$ threads.
The processor trace written by perf turns out to be highly compressable. We
where able to achieve a compression ratio of between 15x and 30x times using lz4
compression algorithm in the fastet compression mode.

\input{tables/benches.tex}
