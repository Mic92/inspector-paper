\section{Design}
\label{sec:algorithms}
In this section, we first formally define the CPG ($\S$\ref{subsec:cpg}), and then present the algorithm to build the CPG ($\S$\ref{subsec:prov-algo}).

\subsection{Concurrent Provenance Graph} 
\label{subsec:cpg}

 We define the {\em
Concurrent Provenance Graph} (CPG)  as a directed acyclic graph $G =
(V,$ $E)$ with vertices ($V$) and edges ($E$). The
vertices of the CPG represent {\em sub-computations}. The edges represent the dependencies between the sub-computations. %We distinguish between three kinds of edges: control, schedule, and data-dependence edges for recording control, schedule, and data dependencies, respectively. 

\myparagraph{Sub-computations} We define a {\em sub-computation}  as the sequence of instructions
executed by a thread between two \pthreads synchronization API calls. %Using the RC memory model, we derive the synchronization and data dependencies at the granularity of a sub-computation.
We further divide each sub-computation as sequence of code thunks, or {\em thunks} to record the control path taken by the executing thread within the sub-computation.


\myparagraph{Dependencies} We distinguish between three kinds of dependencies: control, synchronization, and data dependencies. We next described these dependencies.


%{\em Control edges}  are used to record the intra-thread causal order between sub-computations of the same thread
%based on their execution order, and also control flow within each sub-computation at the granularity of code {\em thunks}. {\em Synchronization edges}   are
%used to record the inter-thread causal order between sub-computations based on the synchronization order between threads. {\em Data dependence edges} are used to establish the {\em update-use relationship} between sub-computations. The update-use relationship exists between two sub-computations if they can be  ordered based on the happens-before relationship, and the write-set of the  precedent  sub-computations intersects with the read-set of the antecedent sub-computations.

 

\myparagraph{I: Control edges} {\em Control edges}  are used to record the intra-thread causal order between sub-computations of the same thread
based on their execution order. Furthermore, we also record all control path taken by the executing thread within each sub-computation during the execution at the granularity of {thunks}.  

We model the execution of thread $t$ as a sequence of sub-computations ($L_t$).  Sub-computations in a thread are totally ordered based on their execution order using a monotonically increasing thunk counter ($\alpha$). We refer a sub-computation of thread $t$ using the counter $\alpha$ as an index in the thread execution sequence ($L_t$), i.e., $L_t[\alpha]$.  

We refer a thunk ($L_t[\alpha].\Delta$) as a sequence of instructions between two successive branches within each subcomputation. We denote a thunk of sub-computation $L_t[\alpha]$ using a counter $\beta$ as an index in the sub-computation as $L_t[\alpha].\Delta[\beta]$.  


\myparagraph{II: Synchronization edges}  {\em Synchronization edges}   are
used to record the inter-thread causal order between sub-computations based on the synchronization order between threads. 
We derive synchronization edges based on the ordering of synchronization operations (also known as a {\em sync schedule}). In particular,  we build on the observation that synchronization primitives can be modeled as {\em acquire} and {\em release} operations. That is,   during synchronization, the synchronization object is {\em released} by one set of threads and subsequently  {\em acquired} by a corresponding set of threads blocked on the object~\cite{fast-track-pldi}. For example, an {\tt unlock(S)} operation releases $S$
and a corresponding {\tt lock(S)} operation acquires it.

%%; a global {\tt
%barrier(S)} operation causes all threads to release $S$ as they reach the
%barrier, and then causes them to re-acquire $S$. Similarly, all other
%synchronization primitives can also be defined using acquire and release
%operations~\cite{djit, fast-track-pldi}, which allows us discuss synchronization only in terms of release and acquire operations.


We derive the partial order based on the happens-before relation
($\rightarrow$)~\cite{djit,fast-track-pldi} between acquire and release operations. In particular, a release
operation happens-before the corresponding acquire operation.
Formally, two sub-computations $L_{(t_1)}[\alpha_1]$ and
$L_{(t_2)}[\alpha_2]$ are ordered by the happens-before relationship ($L_{(t_1)}[\alpha_1] \rightarrow
L_{(t_2)}[\alpha_2]$) if:  ($i$)  they are sub-computations of the
same thread ($t_1 = t_2$), and $L_{(t_1)}[\alpha_1]$ was executed before $L_{(t_2)}[\alpha_2]$; ($ii$)  $L_{(t_1)}[\alpha_1]$  is a release and $L_{(t_2)}[\alpha_2]$ is corresponding acquire on the same synchronization object $S$; ($iii$) due to transitivity if  
 $L_{(t_1)}[\alpha_1] \rightarrow L_{(t_3)}[\alpha_3] $ and $L_{(t_3)}[\alpha_3]  \rightarrow L_{(t_2)}[\alpha_2]$.


\myparagraph{III: Data-dependence edges}  {\em Data dependence edges}  records the flow of data between sub-computations of the same or different threads. 
 We derive the data dependencies between sub-computations using the read/write sets, and recorded partial order in control and synchronization edges. For a sub-computation $L_t[\alpha]$, the {\em read-set}
($L_t[\alpha].R$) and the {\em write-set} ($L_t[\alpha].W$) are the set of
addresses that were respectively read from and written to by
the thread while executing the sub-computation. 

Essentially, data dependence edges establish the {\em update-use relationship} between sub-computations. The update-use relationship exists between two sub-computations if they can be  ordered based on the happens-before relationship, and the write-set of the  precedent  sub-computations transitively intersects with the read-set of the antecedent sub-computations. 



%We derive the read/write sets at the granularity of memory pages for efficiency reasons, as described in~\secref{implementation}.

%(V,E)$ with two types of vertices ($V$) and three types of edges ($E$). The
%vertices of CDDG are sub-computations, which we distinguish between two types of sub-computations:
%independent sub-computations ($V_{Ti}$)  and dependent sub-computations ($V_{Td}$). 



%Given
%the use of the RC model, the happens-before relation
%captures the possible flow of data between sub-computations of the same or different threads. 
 
 
 \subsection{ Provenance Algorithm}
 \label{subsec:prov-algo}

 
 
At high-level, our algorithm records the multithreaded execution to construct the CPG.
%In particular, we derive vertices  by defining
%sub-computations end points between two synchronization operations and recording their respective read/write sets. Control aedges in the CPG are derived by recording the happens-before order between sub-computations. Our algorithm implicitly derives data-dependent edges through the union of the happens-before order between sub-computations and the read/write sets. 
Algorithm~\ref{fig:ithreadsRecord} presents the overview of the provenance algorithm, and details of the subroutines are presented in Algorithm~\ref{fig:ithreadsRecordProc}.
 \input{algorithms/general-record} 

%explain the algorithm using figure.
\myparagraph{Overview}   The provenance algorithm (shown in Algorithm~\ref{fig:ithreadsRecord}) is executed by all threads in parallel. During a thread execution, the thread traces memory accesses on {\tt load/store} instructions, and adds them to the read and
the write set of the executing sub-computation for deriving data dependencies. Additionally, threads also traces all branch instructions, and adds this information to thunks for the executing sub-computation as part of the recording control dependencies.
The thread continues to execute instructions until a synchronization primitive call is made to the \pthreads  library. At the synchronization point, we define the end point for the executing sub-computation. 
Thereafter, we let the thread perform the actual synchronization operation. At synchronization points, the algorithm derives control  and synchronization edges at the granularity of sub-computation by recording the happens-before order between sub-computations. Finally, we start a new sub-computation and repeat the process until the executing thread
terminates. 

 %Our algorithm implicitly derives data-dependent edges through the union of the happens-before order between sub-computations and the read/write sets.


 


%employs run-time techniques to derive the information needed for the CPG. In particular, we derive vertices  by defining sub-computations end points between two synchronization operations. 
   
   
  



 
%get in the gory details of the algorithms.
 \input{algorithms/record-proc}

\myparagraph{Details} For the CPG, control and synchronization dependencies are derived by 
happens-before ordering of sub-computations. To do so, we use vector clocks
($C$)~\cite{Mattern89virtualtime},  a widely used mechanism to generate a partial order of events and to infer causality. Our use of vector
clocks is motivated by its efficiency for recording a partial order between sub-computations in a complete decentralized manner instead of having to serialize all synchronization events in a total order.

In particular, each thread maintains a vector clock, i.e., an array/vector of size equal to the number of threads in the system.  
During a synchronization event, the clock of the thread performing the
acquire operation is updated based on the clock value of the thread performing
the release operation.  More precisely, the vector clock is updated as follows: if a thread $t_2$ acquires the synchronization
object $S$ released by a thread $t_1$, then each entry in $t_2$'s vector is
updated to hold the maximum of its old value and the corresponding value of
$t_1$'s vector at the moment of release.



To implement this mechanism, our algorithm maintains vector clocks for three kinds of
entities: threads,  synchronization objects, and sub-computations.  A {\em thread clock}
($C_t$) for a thread $t$ tracks the local logical time of the thread, which is
incremented each time a new thunk is created. A {\em synchronization clock}
($C_S$) for a synchronization object $S$ acts as a messaging medium  between
threads synchronizing on $S$ to update the thread clock. Finally, a {\em  sub-computation clock}
($L_t[\alpha].C$) determines the position of the sub-computation $L_t[\alpha]$
in the CPG, and is set to the clock value of the thread while executing the
sub-computation . 



Based on the intuition developed so far, we next present the
subroutines used in the recording algorithm (see
Algorithm~\ref{fig:ithreadsRecordProc}). Let $T$ denote the number of threads in the system, which are numbered from $1$ to $T$.  Initially, each thread $t$ initializes (using routine {\tt initThread}($t$)) its monotonically increasing thunk
counter ($\alpha$) and the thread clock ($C_t$) to zero. In addition, vector clocks ($C_S$) of all synchronization objects $S$ are also initialized to zero. 
In the beginning of a new thunk (using routine {\tt startSub-computation()}), the clock value ($C_t$) of the thread $t$ is
updated based on the sub-computation  counter ($\alpha$) to keep track of the local logical time of $t$. The thread clock is updated by assigning the $\alpha$ to $t^{th}$ index of the thread clock $C_t[t]$. The updated value of thread clock ($C_t$)   is also assigned to the sub-computation's clock ($L_t[\alpha].C$). Finally, the read set and the write set
($L_t[T_t].R/W$) of the new sub-computation are initialized to empty set.

During a sub-computation execution, we trace reads and writes  (using routine {\tt onMemoryAccess()}) at the granularity of the
memory pages ({\tt pageID}), and update the respective read/write set
($L_t[T_t].R/W$) of the executing sub-computation.  

Similarly, we also trace branch instructions (using routine {\tt onBranchAccess()}), and update the thunk within the executing sub-computation.

At synchronization points, we define the end of the current sub-computation, and therefore, we increment the sub-computation  counter ($\alpha$) by one. The executing thread performs the synchronization operation (using routine {\tt onSynchronization()}). Recall that in our model, a synchronization operation is either a release or an acquire operation. Therefore, we handle {\tt onSynchronization()} accordingly. If it is a release operation on the synchronization object $S$ by the thread $t$, the 
releasing thread updates the synchronization object's clock ($C_S$) to
hold the maximum of its own clock value ($C_{t}$) and the clock  ($C_S$) of $S$. Then the releasing thread performs the actual release operation  on object $S$. Alternatively, if its an acquire operation then the acquiring thread first performs the acquire  operation on object $S$. After the acquire operation on the synchronization object $S$ by
thread $t$, the acquiring thread updates its own clock ($C_{t}$) to hold the
maximum of the clock value  ($C_S$) of $S$  and its own clock value ($C_{t}$).  
In this way, the synchronization clock ($C_S$) acts as a propagation medium to pass the
vector clock value from the thread doing the release to the thread doing the acquire operation. 



In the end of the provenance algorithm, all sub-computations (along with their read/write sets) have a recorded value of sub-computation's vector clock ($L_t[\alpha].C$). The standard comparison of vector clocks defines the  happens-before partial order, through which causal order is derived between sub-computations.


